{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasMontero/neural_network/blob/master/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixm5oZcuoPbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_circles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt8BKwvWo1Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset creation\n",
        "\n",
        "n = 500\n",
        "p = 2\n",
        "\n",
        "X, Y = make_circles(n_samples = n, factor = 0.5, noise = 0.05)\n",
        "\n",
        "Y = Y[:, np.newaxis]\n",
        "\n",
        "plt.scatter(X[Y[:, 0] == 0 , 0], X[Y[:, 0] == 0 ,1], c='skyblue')\n",
        "plt.scatter(X[Y[:, 0] == 1 , 0], X[Y[:, 0] == 1 ,1], c='salmon')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5cBW87JqfFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network layer class\n",
        "\n",
        "class neural_layer():\n",
        "  def __init__(self, n_connections, n_neurons, activation_function):\n",
        "    \n",
        "    self.activation_function = activation_function\n",
        "    \n",
        "    self.bias = np.random.rand(1, n_neurons)          * 2 -1\n",
        "    self.W = np.random.rand(n_connections, n_neurons) * 2 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxeSmtFjsHfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation functions\n",
        "\n",
        "sigmoid = (lambda x: 1/(1 + np.e ** (-x)),\n",
        "           lambda x: x * (1-x))\n",
        "\n",
        "relu = lambda x : np.maximum(0, x)\n",
        "\n",
        "# representation\n",
        "# _x = np.linspace(-5, 5, 100)\n",
        "# plt.plot(_x, sigmoid[1](_x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh2kqJn2wRHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer0 = neural_layer(p, 4, sigmoid)\n",
        "layer1 = neural_layer(4, 8, sigmoid)\n",
        "# ...\n",
        "\n",
        "def create_neural_network(topology, activation_function):\n",
        "  \n",
        "  neural_network = []\n",
        "  \n",
        "  for i, layer in enumerate(topology[:-1]):\n",
        "    neural_network.append(neural_layer(topology[i], topology[i+1], activation_function))\n",
        "    \n",
        "  return neural_network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0PGyByX63Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topology = [p, 4, 8, 1]\n",
        "\n",
        "neural_network = create_neural_network(topology, sigmoid)\n",
        "\n",
        "cost = (lambda predicted_Y, real_Y: np.mean((predicted_Y - real_Y) ** 2),\n",
        "        lambda predicted_Y, real_Y: (predicted_Y - real_Y))\n",
        "\n",
        "def train(neural_network, X, Y, cost, learning_rate = 0.5, train=True):\n",
        "  \n",
        "  output = [(None, X)]\n",
        "  \n",
        "  # Forward pass\n",
        "  for i, layer in enumerate(neural_network):\n",
        "    weighted_sum = output[-1][1] @ neural_network[i].W + neural_network[i].bias\n",
        "    activation = neural_network[i].activation_function[0](weighted_sum)\n",
        "    \n",
        "    output.append((weighted_sum, activation))\n",
        "      \n",
        "  if train:\n",
        "    \n",
        "    # Backward pass \n",
        "    deltas = []\n",
        "    \n",
        "    for i in reversed(range(0, len(neural_network))):\n",
        "      weighted_sum = output[i+1][0]\n",
        "      activation = output[i+1][1]\n",
        "      \n",
        "      if i == len(neural_network) -1:\n",
        "        # Calculate delta of last layer\n",
        "        deltas.insert(0, cost[1](activation, Y) * neural_network[i].activation_function[1](activation))\n",
        "      else:\n",
        "        # Calculate previous layer delta\n",
        "        deltas.insert(0, deltas[0] @ _X.T * neural_network[i].activation_function[1](activation))\n",
        "        \n",
        "      _X = neural_network[i].W \n",
        "                      \n",
        "      # Gradient descent\n",
        "\n",
        "      neural_network[i].bias = neural_network[i].bias - np.mean(deltas[0], axis=0, keepdims=True) * learning_rate \n",
        "      neural_network[i].W = neural_network[i].W - output[i][1].T @ deltas[0] * learning_rate \n",
        "\n",
        "  return output[-1][1]               \n",
        "                      \n",
        "train(neural_network, X, Y, cost, 0.5)\n",
        "print('')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}